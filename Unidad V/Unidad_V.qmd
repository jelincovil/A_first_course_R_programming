---
title: "Unidad V: Generación de datos desde modelos de probabilidad"
format: html
editor: visual
---

### **Sesión 1: Martes 10-06-2026**

# Simulación de datos desde modelos de probabilidad

::: preponderante-frase
**Principio: Los datos iniciales me dan una idea del proceso aleatorio que los generó. Con este proceso puedo generar datos aumentados para extraer el máximo de información**.
:::

-   **1 Estudios de modelos generando datos en base de hipótesis**
-   **2 Aumento y robustez de datos**
-   **3 Estudio de propiedades estadísticas**
-   **4 Verificación y validación de métodos estadísticos**

```{mermaid}

graph TD
    A[Generación de Datos con Modelos] --> C{Estudios de Modelos e Hipótesis};
    A --> D{Aumento y Robustez de Datos};
    A --> E{Estudio de Propiedades Estadísticas};
    A --> F{Verificación y Validación de Métodos};

    C -- "Proporciona bases para" --> D;
    D -- "Alimenta análisis para" --> E;
    E -- "Ayuda a la fiabilidad de" --> F;
    F -- "Refina Modelos/Supuestos para" --> C;
    C -- "Guía la especificación del modelo" --> E;
    D -- "Mejora la calidad de la estimación" --> E;
    E -- "Fundamental para inferencia" --> F;

```

La simulación de datos, anclada en modelos probabilísticos, es una piedra angular en la estadística moderna. Permite explorar fenómenos aleatorios sin información empírica, facilitando la formulación de hipótesis y el diseño experimental. Además, es crucial para aumentar y robustecer análisis con datos escasos, permitiendo un estudio profundo de la variabilidad y estabilidad de las estimaciones. La técnica también es indispensable para la estimación de propiedades de estadísticos, como las distribuciones de muestreo y los intervalos de confianza, mediante métodos como Monte Carlo. Finalmente, la simulación es vital para verificar y validar la robustez y precisión de los métodos estadísticos bajo diversas condiciones controladas.

## **1. Estudios de modelos generando datos en base de hipótesis**

Este principio recomienda explorar fenómenos aleatorios sin datos reales, facilitando la formulación y prueba de hipótesis sobre su comportamiento y el diseño de experimentos futuros. Es esencial para obtener una comprensión inicial del sistema antes de cualquier recolección empírica.

## **2. Aumento y Robustez de Datos**

La simulación es útil para enriquecer análisis con datos empíricos escasos, generando información a partir de modelos ajustados. Esto permite estudiar la variabilidad y estabilidad de las estimaciones y obtener conclusiones más robustas a partir de conjuntos de datos limitados.

## **3. Estudio de Propiedades Estadísticas**

Si la hipótesis sobre mis datos fuesen ciertas ¿Qué propiedades tiene cada método estadístico? Este principio se centra en estimar propiedades de estadísticos con distribuciones desconocidas analíticamente. Mediante simulaciones como Monte Carlo, se pueden aproximar distribuciones de muestreo, calcular intervalos de confianza y determinar la potencia de pruebas, vital para la inferencia.

## **4. Verificación y Validación de Métodos**

Si algunas de mis supuestos sobre el modelo no fuesen verdaderos, ¿cómo afectaria esto a mis métodos estadísticos? La simulación es clave para probar la validez y robustez de los métodos estadísticos. Permite verificar si un método produce resultados precisos y confiables bajo condiciones controladas y diversos escenarios, asegurando su aplicabilidad y exactitud.

# **Ejemplos**

## **5. Ejemplo de exploración y Generación de Hipótesis**

### Generador de realizaciones de variables aleatorias de una dimensión

Este apartado,  generamos datos de distribuciones de probabilidad univariadas comunes en R, lo que permite explorar hipótesis sobre la naturaleza de una sola variable.

#### **1. Generando números uniformes** (`runif()`)

La función `runif(n, min=0, max=1)` genera `n` números aleatorios de una distribución uniforme. Cada valor en el rango `[min, max]` tiene la misma probabilidad de ser generado.

Densidad asociada: dunif(x, min=0, max=1)


```{r}
# Generar 5 números uniformes entre 0 y 1
runif(5)

# Generar 10 números uniformes entre 10 y 20
runif(10, min = 10, max = 20)

# Visualizar la densidad de una muestra uniforme
hist(runif(1000, min = 0, max = 1), freq = FALSE,
     main = "Densidad de Muestra Uniforme", xlab = "Valor")
curve(dunif(x, min = 0, max = 1), add = TRUE, col = "red", lwd = 2)

```


#### **2. Generando números binomiales** (`rbinom()`)

La función `rbinom(n, size, prob)` genera `n` números aleatorios de una distribución binomial. Esta distribución describe el número de éxitos (`x`) en un número fijo de ensayos (`size`), donde cada ensayo tiene una probabilidad de éxito (`prob`).

Densidad (función de masa de probabilidad) asociada: dbinom(x, size, prob)

Código de ejemplo:

```
# Simular el número de caras al lanzar una moneda 10 veces, repetido 5 veces
rbinom(n = 5, size = 10, prob = 0.5)

# Simular el número de estudiantes que aprueban de 20, si la prob de aprobar es 0.7
rbinom(n = 1, size = 20, prob = 0.7)

# Visualizar la densidad de una muestra binomial
sim_binomial <- rbinom(n = 1000, size = 10, prob = 0.5)
barplot(table(sim_binomial) / length(sim_binomial),
        main = "Densidad de Muestra Binomial", xlab = "Número de Éxitos", ylab = "Probabilidad")
points(x = 0:10, y = dbinom(0:10, size = 10, prob = 0.5), col = "red", pch = 16) # Puntos de densidad

```

----------

#### **3. Generando números de Poisson** (`rpois()`)

La función `rpois(n, lambda)` genera `n` números aleatorios de una distribución de Poisson, donde `lambda` es la tasa promedio de ocurrencias en un intervalo fijo de tiempo o espacio.

Densidad (función de masa de probabilidad) asociada: dpois(x, lambda)

```{r}
# Simular el número de llamadas recibidas en una hora, si el promedio es 5 llamadas/hora
rpois(n = 1, lambda = 5)

# Simular el número de defectos por metro de tela, si el promedio es 0.2 defectos/metro
rpois(n = 10, lambda = 0.2)

# Visualizar la densidad de una muestra de Poisson
sim_poisson <- rpois(n = 1000, lambda = 3)
barplot(table(sim_poisson) / length(sim_poisson),
        main = "Densidad de Muestra de Poisson", xlab = "Número de Eventos", ylab = "Probabilidad")
points(x = 0:max(sim_poisson), y = dpois(0:max(sim_poisson), lambda = 3), col = "red", pch = 16)

```

#### **4. Generando números normales** (`rnorm()`)

La función `rnorm(n, mean=0, sd=1)` genera `n` números aleatorios de una distribución normal (gaussiana) con una media (`mean`) y desviación estándar (`sd`) especificadas.

Densidad asociada: dnorm(x, mean=0, sd=1)

```{r}
# Simular la altura de 10 personas, con media 170 cm y desviación estándar 5 cm
rnorm(n = 10, mean = 170, sd = 5)

# Generar 1000 valores de una distribución normal estándar y visualizar su densidad
hist(rnorm(1000, mean = 0, sd = 1), freq = FALSE,
     main = "Densidad de Muestra Normal", xlab = "Valor")
curve(dnorm(x, mean = 0, sd = 1), add = TRUE, col = "red", lwd = 2)

```


#### **5. Generando números exponenciales** (`rexp()`)

La función `rexp(n, rate=1)` genera `n` números aleatorios de una distribución exponencial, donde `rate` (tasa) es 1/media.

Densidad asociada: dexp(x, rate=1)

```{r}
# Simular el tiempo de espera hasta la siguiente llegada de un cliente, si la tasa es 0.5 llegadas/minuto
rexp(n = 1, rate = 0.5)

# Generar 20 tiempos de falla de un componente con una tasa de 0.1 fallas/hora
rexp(n = 20, rate = 0.1)

# Visualizar la densidad de una muestra exponencial
hist(rexp(1000, rate = 1), freq = FALSE,
     main = "Densidad de Muestra Exponencial", xlab = "Valor")
curve(dexp(x, rate = 1), add = TRUE, col = "red", lwd = 2)

```


#### **6. Asegurando la reproducibilidad** (`set.seed()`)

Para que tus simulaciones generen los mismos resultados cada vez que se ejecutan (esencial para la verificación de hipótesis y compartir código reproducible), se usa `set.seed()`.

```{r}
# Sin set.seed(), cada ejecución dará números diferentes
rnorm(3)
rnorm(3)

# Con set.seed(), siempre obtendrás los mismos números
set.seed(42)
rnorm(3) # La primera ejecución con esta semilla
set.seed(42)
rnorm(3) # La segunda ejecución con la misma semilla da los mismos resultados
```


### Generador de realizaciones de variables aleatorias de dos dimensiones

Simular datos bidimensionales nos permite explorar cómo dos variables podrían estar relacionadas o no. Esto es clave para probar ideas sobre cómo funcionan los sistemas.



#### **1. Variables Normales Correlacionadas**

 Generar dos variables normales con una relación específica.

```{r}
set.seed(123) # Para reproducibilidad
n <- 100     # Cantidad de puntos
rho <- 0.7   # Correlación deseada

# Generar variables normales independientes
z1 <- rnorm(n)
z2 <- rnorm(n)

# Transformar para crear correlación (ej. altura y peso)
altura <- 170 + 10 * z1
peso <- 70 + 10 * (rho * z1 + sqrt(1 - rho^2) * z2)

# Ver datos y correlación
datos_simulados <- data.frame(Altura = altura, Peso = peso)
print(head(datos_simulados))
print(cor(datos_simulados$Altura, datos_simulados$Peso))

# Visualizar la relación
plot(datos_simulados$Altura, datos_simulados$Peso,
     main = "Altura vs. Peso (Correlacionados)",
     xlab = "Altura (cm)", ylab = "Peso (kg)")

```


#### **2. Relación Lineal con Ruido (Regresión Simple)**

 Simular una variable que depende linealmente de otra, con algo de aleatoriedad.

```{r}
set.seed(456) # Para reproducibilidad
n <- 50      # Cantidad de puntos

# Variable independiente (ej. publicidad)
publicidad <- runif(n, min = 10, max = 100)

# Variable dependiente (ej. ventas = 200 + 3*publicidad + ruido)
ventas <- 200 + 3 * publicidad + rnorm(n, mean = 0, sd = 50)

# Ver datos
datos_simulados_reg <- data.frame(Publicidad = publicidad, Ventas = ventas)
print(head(datos_simulados_reg))

# Visualizar la relación
plot(datos_simulados_reg$Publicidad, datos_simulados_reg$Ventas,
     main = "Ventas vs. Publicidad (Relación Lineal)",
     xlab = "Inversión en Publicidad", ylab = "Ventas")
abline(lm(Ventas ~ Publicidad, data = datos_simulados_reg), col = "red")

```


#### **3. Variables Independientes (sin relación)**

 Generar dos variables donde una no afecta a la otra.

```{r}
set.seed(789) # Para reproducibilidad
n <- 100     # Cantidad de puntos

# Variable 1 (ej. examen 1)
examen_1 <- rnorm(n, mean = 70, sd = 10)

# Variable 2 (ej. tiempo de traslado)
tiempo_traslado <- runif(n, min = 5, max = 60)

# Ver datos y correlación (cercana a cero)
datos_independientes <- data.frame(Examen1 = examen_1, TiempoTraslado = tiempo_traslado)
print(head(datos_independientes))
print(cor(datos_independientes$Examen1, datos_independientes$TiempoTraslado))

# Visualizar la ausencia de relación
plot(datos_independientes$Examen1, datos_independientes$TiempoTraslado,
     main = "Calificación Examen vs. Tiempo Traslado (Independientes)",
     xlab = "Calificación Examen 1", ylab = "Tiempo de Traslado (min)")

```

#### **4. Relación No Lineal (Exponencial)**

 Simular una variable que crece o decrece exponencialmente con respecto a otra.

```{r}
set.seed(1011) # Para reproducibilidad
n <- 70      # Cantidad de puntos

# Variable independiente (ej. tiempo)
tiempo <- seq(0, 10, length.out = n)

# Variable dependiente: crecimiento exponencial con ruido
poblacion <- 50 * exp(0.5 * tiempo) + rnorm(n, mean = 0, sd = 400)

# Ver datos
datos_exponenciales <- data.frame(Tiempo = tiempo, Poblacion = poblacion)
print(head(datos_exponenciales))

# Visualizar la relación exponencial
plot(datos_exponenciales$Tiempo, datos_exponenciales$Poblacion,
     main = "Crecimiento Poblacional (Exponencial)",
     xlab = "Tiempo", ylab = "Tamaño de Población")
curve(50 * exp(0.5 * x), add = TRUE, col = "red", lwd = 2, lty = 2) # Curva teórica

```

### Simulación de cadenas de Markov


Una **cadena de Markov** es un proceso estocástico que describe una secuencia de eventos donde la probabilidad de cada evento futuro depende únicamente del estado actual, y no de la secuencia de eventos que lo precedieron. Matemáticamente, esto se conoce como la **propiedad de Markov**. Se define por un conjunto de estados y una **matriz de probabilidades de transición** P, donde Pij​ es la probabilidad de pasar del estado i al estado j.
$$
\{X_1, X_{2}, X_{3}, \ldots, X_n\} = \{X_n\}
$$
$$
P(X_{n+1}=j|X_{n}=i,  X_{n-1}=s, \ldots, X_{1}=f) = P(X_{n+1}=j|X_{n}=i)=P_{ij}
$$


### **Simulación de Cadenas de Markov: Ejemplos con R**

Las cadenas de Markov modelan sistemas que cambian de estado discretamente a lo largo del tiempo, donde el próximo estado solo depende del estado actual. Esto es útil para explorar la dinámica de sistemas con transiciones probabilísticas.


#### **1. Clima Simple: Soleado o Lluvioso**

Simular la secuencia de estados del clima (soleado o lluvioso) día a día, basándose en la probabilidad de que cambie o permanezca igual.

| | Soleado | Lluvioso |
|---|---|---|
| **Soleado** | 0.8 | 0.2 |
| **Lluvioso** | 0.3 | 0.7 |

```{r}
set.seed(101) # Para reproducibilidad

# Definir los posibles estados del clima
estados <- c("Soleado", "Lluvioso")

# Matriz de probabilidades de transición
# P(Soleado -> Soleado) = 0.8, P(Soleado -> Lluvioso) = 0.2
# P(Lluvioso -> Soleado) = 0.3, P(Lluvioso -> Lluvioso) = 0.7
matriz_transicion <- matrix(c(0.8, 0.2,  # Desde "Soleado"
                              0.3, 0.7), # Desde "Lluvioso"
                            nrow = 2, byrow = TRUE,
                            dimnames = list(estados, estados))

n_dias <- 30 # Simular 30 días
estado_actual <- "Soleado" # Empezamos con un día soleado
secuencia_clima <- character(n_dias)
secuencia_clima[1] <- estado_actual

# Simular la cadena de Markov día a día
for (i in 2:n_dias) {
  # Obtener las probabilidades de transición desde el estado actual
  prob_siguientes <- matriz_transicion[estado_actual, ]
  
  # Muestrear el siguiente estado basado en esas probabilidades
  estado_actual <- sample(estados, size = 1, prob = prob_siguientes)
  secuencia_clima[i] <- estado_actual
}

print("Secuencia de Clima Simulada:")
print(secuencia_clima)

# Frecuencia de estados en la simulación
print("Frecuencia de estados:")
print(table(secuencia_clima))

```

#### **2. Movimiento de un Cliente entre Secciones de una Tienda**

Simular el recorrido de un cliente por diferentes secciones de una tienda (e.g., Entrada, Ropa, Comida, Salida), con probabilidades de moverse de una a otra.

```{r}
set.seed(202) # Para reproducibilidad

# Definir las secciones de la tienda
secciones <- c("Entrada", "Ropa", "Comida", "Salida")

# Matriz de probabilidades de transición
# Ejemplo:
# De Entrada: 50% Ropa, 30% Comida, 20% Salida
# De Ropa: 10% Entrada, 40% Ropa, 30% Comida, 20% Salida
# De Comida: 0% Entrada, 20% Ropa, 50% Comida, 30% Salida
# De Salida: 100% Salida (estado absorbente)
matriz_tienda <- matrix(c(0.0, 0.5, 0.3, 0.2, # Desde Entrada
                          0.1, 0.4, 0.3, 0.2, # Desde Ropa
                          0.0, 0.2, 0.5, 0.3, # Desde Comida
                          0.0, 0.0, 0.0, 1.0), # Desde Salida
                        nrow = 4, byrow = TRUE,
                        dimnames = list(secciones, secciones))

n_pasos_max <- 15 # Simular hasta 15 movimientos
trayectoria_cliente <- character(n_pasos_max)
estado_actual <- "Entrada" # El cliente siempre empieza en la Entrada
trayectoria_cliente[1] <- estado_actual

# Simular la trayectoria
for (i in 2:n_pasos_max) {
  if (estado_actual == "Salida") { # Si ya salió, se queda en "Salida"
    trayectoria_cliente[i] <- "Salida"
    next
  }
  
  prob_siguientes <- matriz_tienda[estado_actual, ]
  estado_actual <- sample(secciones, size = 1, prob = prob_siguientes)
  trayectoria_cliente[i] <- estado_actual
}

print("Trayectoria del Cliente Simulada:")
print(trayectoria_cliente)

# Contar cuántas veces estuvo en cada sección
print("Visitas por sección:")
print(table(trayectoria_cliente))

```

### Métodos de aceptación y rechazo

Este método simula valores de distribuciones complejas (como las bimodales) usando una distribución más simple. Se generan propuestas de la simple y se aceptan o rechazan según una regla de probabilidad, permitiendo muestrear formas inusuales.

Acepta una muestra propuesta x si:

$$
U<M⋅g(x)f(x)​
$$
Donde:

-   U: Es un número aleatorio generado de una distribución uniforme entre 0 y 1 ($U∼Unif(0,1)$).
-   f(x): Es la función de densidad de probabilidad (FDP) de la **distribución objetivo** de la que quieres muestrear.
-   g(x): Es la FDP de la **distribución propuesta** (o envolvente), de la cual es fácil generar muestras.
-   M: Es una constante tal que M⋅g(x)≥f(x) para todo x. Es decir, M⋅g(x) debe ser una "envolvente" de f(x).

El **muestreo de aceptación/rechazo** genera números aleatorios de una distribución objetivo f(x) (compleja) usando una distribución propuesta g(x) (fácil de muestrear) y una constante M tal que M⋅g(x)≥f(x). Se acepta una muestra propuesta x (generada de g(x)) si un número uniforme aleatorio U (entre 0 y 1) es menor que la razón f(x)/(M⋅g(x)), de lo contrario se rechaza, asegurando que los puntos aceptados provengan efectivamente de la distribución f(x).

Hipótesis: Queremos muestrear una mezcla de dos distribuciones normales (una densidad bimodal), con la siguiente función de densidad:

$$
f(x)=w1​⋅N(x∣μ1​,σ1​)+w2​⋅N(x∣μ2​,σ2​) donde w1​+w2​=1.
$$

En nuestro ejemplo, $f(x)=0.4⋅N(x∣μ1​=2,σ1​=0.5)+0.6⋅N(x∣μ2​=5,σ2​=1$).

```{r}
# función de densidad bimodal
f_x <- function(x) {
  0.4 * dnorm(x, mean = 2, sd = 0.5) + 0.6 * dnorm(x, mean = 5, sd = 1)
}

#  la curva de densidad
curve(f_x(x), from = -1, to = 8,
      main = "Densidad Bimodal",
      xlab = "x", ylab = "f(x)",
      col = "blue", lwd = 2)

```

```{r}
set.seed(1233) # Para reproducibilidad

# 1. Densidad objetivo (la que queremos muestrear: bimodal)
target_density <- function(x) {
  0.4 * dnorm(x, mean = 2, sd = 0.5) + 0.6 * dnorm(x, mean = 5, sd = 1)
}

# 2. Densidad propuesta (una normal simple que la "envuelve")
proposal_density <- function(x) {
  dnorm(x, mean = 3.5, sd = 2)
}

# 3. Encontrar M (constante para escalar la propuesta)
x_range <- seq(-2, 8, length.out = 1000)
M <- max(target_density(x_range) / proposal_density(x_range))
cat("Valor de M:", M, "\n") # M debe ser > 1

# 4. Generar muestras por Aceptación-Rechazo
n_muestras_deseadas <- 5000
muestras_aceptadas <- numeric(n_muestras_deseadas)
conteo_aceptadas <- 0
conteo_propuestas <- 0

while (conteo_aceptadas < n_muestras_deseadas) {
  propuesta_x <- rnorm(1, mean = 3.5, sd = 2) # Generar de la propuesta
  u <- runif(1) # Valor para decisión

  # Aceptar si la propuesta está "debajo" de la densidad objetivo ajustada
  if (proposal_density(propuesta_x) > 0 && u < (target_density(propuesta_x) / (M * proposal_density(propuesta_x)))) {
    conteo_aceptadas <- conteo_aceptadas + 1
    muestras_aceptadas[conteo_aceptadas] <- propuesta_x
  }
  conteo_propuestas <- conteo_propuestas + 1 # Contar todas las propuestas
}

cat("Propuestas generadas:", conteo_propuestas, "\n")
cat("Eficiencia:", n_muestras_deseadas / conteo_propuestas, "\n")

# 5. Visualizar resultados
hist(muestras_aceptadas, breaks = 50, freq = FALSE,
     main = "Muestras Bimodales por Aceptación-Rechazo",
     xlab = "Valor de X", ylab = "Densidad", col = "lightblue",
     ylim = c(0,0.43))

# Añadir curvas para comparación
curve(target_density(x), add = TRUE, col = "red", lwd = 2) # Objetivo
curve(M * proposal_density(x), add = TRUE, col = "darkgreen", lty = 2, lwd = 2) # Propuesta escalada

legend("topright", legend = c("Densidad Objetivo", "Densidad Estimada"),
       col = c("red", "darkgreen"), lty = c(1, 2), lwd = 2)

```



## **6. Ejemplo del aumento y robustez de los datos**

### Estimación de la Varianza y Construcción de Intervalos de Confianza via Bootstrapping

Cuando se tiene una muestra limitada y la distribución de un estadístico (como la mediana, un coeficiente de regresión complejo o la razón de riesgo) no es conocida analíticamente o no cumple con las suposiciones paramétricas, se utiliza el **Bootstrapping**. Se generan miles de "nuevas muestras" remuestreando con reemplazo de la muestra original, simulando la extracción de la población. Esto permite estudiar la distribución empírica del estadístico y estimar su variabilidad (errores estándar) y construir intervalos de confianza robustos, extrayendo máxima riqueza de la muestra escasa.

### Evaluación Robusta del Rendimiento de Modelos via Cross-Validation

Con conjuntos de datos limitados, la **validación cruzada (k-fold Cross-Validation)** es una técnica fundamental. Se divide el dataset original en múltiples subconjuntos, utilizando unos para entrenar el modelo y otros para probarlo repetidamente. Aunque no genera datos "nuevos" en el sentido estricto, simula el rendimiento del modelo en diferentes particiones de la información existente, permitiendo obtener una estimación más estable y robusta de la capacidad de generalización del modelo, mitigando la variabilidad de una única partición de entrenamiento/prueba.

### Estudio de las Propiedades de Estimadores en Muestras Pequeñas via Simulación Monte Carlo

Al desarrollar nuevos estimadores estadísticos o al aplicar estimadores conocidos en escenarios con muestras limitadas donde la teoría asintótica no aplica bien, se emplean **estudios de Monte Carlo**. Se especifica un modelo de generación de datos conocido (una hipótesis sobre la "verdadera" distribución), se simulan numerosos conjuntos de datos a partir de este, y se aplica el estimador a cada uno.

### Manejo de Datos Faltantes Imputation Multiple:

Cuando un dataset es limitado y presenta valores perdidos, se puede utilizar la **imputación múltiple**. Esta técnica implica ajustar un modelo estadístico (basado en los datos observados) para predecir y generar múltiples conjuntos de valores plausibles para los datos faltantes.

------------------------------------------------------------------------

### **Sesión 2: Sabado 14-06-2026**

## **7. Ejemplo de estudio de propiedades estadísticas**

## **8. Ejemplo de verificación y validación de métodos**
